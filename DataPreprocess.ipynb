{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c5e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from functools import reduce\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f87df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(\"./data/tianchi_mobile_recommend_train_user.csv\")\n",
    "item_data = pd.read_csv(\"./data/tianchi_mobile_recommend_train_item.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61ba81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data[\"time\"] = pd.to_datetime(user_data[\"time\"], format=\"%Y%m%d %H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b8bf859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    reduce the memory of dataframe \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f99b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of actions of users on each item\n",
    "def item_action_total_counts(data):\n",
    "    data[\"item_action_total_counts\"] = 0\n",
    "    feature = data[[\"item_id\", \"item_action_total_counts\"]].groupby([\"item_id\"]).count()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffd48d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of operations of each user\n",
    "def user_operations_total_counts(data):\n",
    "    data[\"user_operations_total_counts\"] = 0\n",
    "    feature = data[[\"user_id\", \"user_operations_total_counts\"]].groupby([\"user_id\"]).count()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bff75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time difference between first time a user checks the item and the time a user buy the item\n",
    "def check_to_buy(data):\n",
    "    buy_user_item_id = data[data[\"behavior_type\"] == 4][[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    data = pd.merge(buy_user_item_id, data, how=\"left\", on=[\"user_id\", \"item_id\"])[[\"user_id\", \"item_id\", \"behavior_type\", \"time\"]]\n",
    "    first_time_check = data[data[\"behavior_type\"] == 1].groupby([\"user_id\", \"item_id\"],as_index=False).min()\n",
    "    first_time_check.rename(columns={\"time\": \"first_time_check\"}, inplace=True)\n",
    "    first_time_buy = data[data[\"behavior_type\"] == 4].groupby([\"user_id\", \"item_id\"],as_index=False).min()\n",
    "    first_time_buy.rename(columns={\"time\": \"first_time_buy\"}, inplace=True)\n",
    "    feature = pd.merge(first_time_buy, first_time_check, how='left',on=[\"user_id\", \"item_id\"])\n",
    "    feature[\"check_to_buy_time_difference\"] = (feature[\"first_time_buy\"] - feature[\"first_time_check\"]).dt.total_seconds()\n",
    "    feature = feature.drop(columns=['behavior_type_x','first_time_buy','behavior_type_y','first_time_check'])\n",
    "    feature = feature[feature['check_to_buy_time_difference'].notnull()]\n",
    "    #feature = feature.loc[(feature != 0).all(axis=1), :]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf14092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time difference between first time a user add the item to cart and the time a user buy the item\n",
    "def add_to_buy(data):\n",
    "    buy_user_item_id = data[data[\"behavior_type\"] == 4][[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    data = pd.merge(buy_user_item_id, data, how=\"left\", on=[\"user_id\", \"item_id\"])[[\"user_id\", \"item_id\", \"behavior_type\", \"time\"]]\n",
    "    first_time_add = data[data[\"behavior_type\"] == 3].groupby([\"user_id\", \"item_id\"],as_index=False).min()\n",
    "    first_time_add.rename(columns={\"time\": \"first_time_add\"}, inplace=True)\n",
    "    first_time_buy = data[data[\"behavior_type\"] == 4].groupby([\"user_id\", \"item_id\"],as_index=False).min()\n",
    "    first_time_buy.rename(columns={\"time\": \"first_time_buy\"}, inplace=True)\n",
    "    feature = pd.merge(first_time_buy, first_time_add, how='left',on=[\"user_id\", \"item_id\"])\n",
    "    feature[\"add_to_buy_time_difference\"] = (feature[\"first_time_buy\"] - feature[\"first_time_add\"]).dt.total_seconds()\n",
    "    feature = feature.drop(columns=['behavior_type_x','first_time_buy','behavior_type_y','first_time_add'])\n",
    "    feature = feature[feature['add_to_buy_time_difference'].notnull()]\n",
    "    #feature = feature.loc[(feature != 0).all(axis=1), :]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bf30832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the time difference between first time a user save the item and the time a user buy the item\n",
    "def save_to_buy(data):\n",
    "    buy_user_item_id = data[data[\"behavior_type\"] == 4][[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    data = pd.merge(buy_user_item_id, data, how='left', on=[\"user_id\", \"item_id\"])[[\"user_id\", \"item_id\", \"behavior_type\", \"time\"]]\n",
    "    first_time_save = data[data[\"behavior_type\"] == 2].groupby([\"user_id\", \"item_id\"],as_index=False).min()\n",
    "    first_time_save.rename(columns={\"time\": \"first_time_save\"}, inplace=True)\n",
    "    first_time_buy = data[data[\"behavior_type\"] == 4].groupby([\"user_id\", \"item_id\"],as_index=False).min()\n",
    "    first_time_buy.rename(columns={\"time\": \"first_time_buy\"}, inplace=True)\n",
    "    feature = pd.merge(first_time_buy, first_time_save, how='left',on=[\"user_id\", \"item_id\"])\n",
    "    feature[\"save_to_buy_time_difference\"] = (feature[\"first_time_buy\"] - feature[\"first_time_save\"]).dt.total_seconds()\n",
    "    feature = feature.drop(columns=['behavior_type_x','first_time_buy','behavior_type_y','first_time_save'])\n",
    "    feature = feature[feature['save_to_buy_time_difference'].notnull()]\n",
    "    #feature = feature.loc[(feature != 0).all(axis=1), :]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d759e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_type_counts(data, choose_behavior_type, name):\n",
    "    data[name] = 1\n",
    "    feature = data[data[\"behavior_type\"] == choose_behavior_type][[\"item_id\", name]].groupby([\"item_id\"]).count()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61cf4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_type_counts(data, choose_behavior_type, name):\n",
    "    data[name] = 1\n",
    "    feature = data[data[\"behavior_type\"] == choose_behavior_type][[\"user_id\", name]].groupby([\"user_id\"]).count()\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "848d4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_user(table1, table2):\n",
    "    data = pd.merge(table1, table2, on=\"user_id\")\n",
    "    return data\n",
    "def merge_item(table1, table2):\n",
    "    data = pd.merge(table1, table2, on=\"item_id\")\n",
    "    return data\n",
    "def merge_time_difference(table1, table2):\n",
    "    data = table2.merge(table1, how = 'inner', on = ['user_id', 'item_id'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b430b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(predict_date):\n",
    "    train_data = user_data[user_data[\"time\"] < predict_date]\n",
    "    i1 = item_action_total_counts(train_data)\n",
    "    i2 = item_type_counts(train_data, 1, \"item_check_counts\")\n",
    "    i3 = item_type_counts(train_data, 2, \"item_save_counts\")\n",
    "    i4 = item_type_counts(train_data, 3, \"item_add_counts\")\n",
    "    i5 = item_type_counts(train_data, 4, \"item_buy_counts\")\n",
    "\n",
    "    u1 = user_operations_total_counts(train_data)\n",
    "    u2 = user_type_counts(train_data, 1, \"user_check_counts\")\n",
    "    u3 = user_type_counts(train_data, 2, \"user_save_counts\")\n",
    "    u4 = user_type_counts(train_data, 3, \"user_add_counts\")\n",
    "    u5 = user_type_counts(train_data, 4, \"user_buy_counts\")\n",
    "    \n",
    "    t1 = check_to_buy(train_data)\n",
    "    t2 = save_to_buy(train_data)    \n",
    "    t3 = add_to_buy(train_data)    \n",
    "    \n",
    "    train_data = train_data[[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    \n",
    "    train_data = ft.reduce(lambda tb1, tb2: pd.merge(tb1, tb2, how='left', on=\"item_id\"),[train_data, i1, i2, i3, i4, i5])\n",
    "    train_data = ft.reduce(lambda tb1, tb2: pd.merge(tb1, tb2, how='left', on=\"user_id\"), [train_data, u1, u2, u3, u4, u5])\n",
    "    train_data = ft.reduce(lambda tb1, tb2: tb2.merge(tb1, how = 'inner', on = ['user_id', 'item_id']), [train_data, t1,t2,t3])\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a1b17c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user action count\n",
    "def user_action_total_counts(data):\n",
    "    data[\"user_action_total_counts\"] = 1\n",
    "    feature = data[[\"user_id\", \"user_action_total_counts\"]].groupby([\"user_id\"], as_index=False).count()\n",
    "    return feature\n",
    "#user type count\n",
    "def user_type_counts(data, type, name):\n",
    "    data[name] = 1\n",
    "    feature = data[data[\"behavior_type\"] == type][[\"user_id\", name]].groupby([\"user_id\"], as_index=False).count()\n",
    "    return feature\n",
    "\n",
    "# the number of item actions\n",
    "def item_action_total_counts(data):\n",
    "    data[\"item_action_total_counts\"] = 1\n",
    "    feature = data[[\"item_id\", \"item_action_total_counts\"]].groupby([\"item_id\"], as_index=False).count()\n",
    "    return feature\n",
    "# user counts\n",
    "def item_total_user_counts(data):\n",
    "    data = data[[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    data[\"item_total_user_counts\"] = 1\n",
    "    feature = data[[\"item_id\", \"item_total_user_counts\"]].groupby([\"item_id\"], as_index=False).count()\n",
    "    return feature\n",
    "# type counts\n",
    "def item_type_counts(data, type, name):\n",
    "    data[name] = 1\n",
    "    feature = data[data[\"behavior_type\"] == type][[\"item_id\", name]].groupby([\"item_id\"], as_index=False).count()\n",
    "    return feature\n",
    "\n",
    "# counts for each category\n",
    "def category_action_total_counts(data):\n",
    "    data[\"category_action_total_counts\"] = 1\n",
    "    feature = data[[\"item_category\", \"category_action_total_counts\"]].groupby([\"item_category\"], as_index=False).count()\n",
    "    return feature\n",
    "# for each category: user counts\n",
    "def category_total_user_counts(data):\n",
    "    data = data[[\"user_id\", \"item_category\"]].drop_duplicates()\n",
    "    data[\"category_total_user_counts\"] = 1\n",
    "    feature = data[[\"item_category\", \"category_total_user_counts\"]].groupby([\"item_category\"], as_index=False).count()\n",
    "    return feature\n",
    "# type counts\n",
    "def category_type_counts(data, type, name):\n",
    "    data[name] = 1\n",
    "    feature = data[data[\"behavior_type\"] == type][[\"item_category\", name]].groupby([\"item_category\"], as_index=False).count()\n",
    "    return feature\n",
    "\n",
    "\n",
    "def user_item_action_total_counts(data):\n",
    "    data[\"user_item_action_total_counts\"] = 1\n",
    "    feature = data[[\"user_id\", \"item_id\", \"user_item_action_total_counts\"]].groupby([\"user_id\", \"item_id\"], as_index=False).count()\n",
    "    return feature\n",
    "\n",
    "def user_item_type_counts(data, type, name):\n",
    "    data[name] = 1\n",
    "    feature = data[data[\"behavior_type\"] == type][[\"user_id\", \"item_id\", name]].groupby([\"user_id\", \"item_id\"], as_index=False).count()\n",
    "    return feature\n",
    "\n",
    "def user_item_last_type_time(data, type, name):\n",
    "    feature = data[data[\"behavior_type\"] == type][[\"user_id\", \"item_id\", \"time\"]].groupby([\"user_id\", \"item_id\"], as_index=False).max()\n",
    "    feature.rename(columns={\"time\": name}, inplace=True)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def user_item_look_to_buy(data):\n",
    "    buy_user_item = data[data[\"behavior_type\"] == 4][[\"user_id\", \"item_id\"]].drop_duplicates()\n",
    "    data = pd.merge(buy_user_item, data, how=\"left\", on=[\"user_id\", \"item_id\"])[[\"user_id\", \"item_id\", \"behavior_type\", \"time\"]]\n",
    "    earliest_look = data[data[\"behavior_type\"] == 1].groupby([\"user_id\", \"item_id\"], as_index=False).min()\n",
    "    earliest_look.rename(columns={\"time\": \"earliest_look_time\"}, inplace=True)\n",
    "    earliest_buy = data[data[\"behavior_type\"] == 4].groupby([\"user_id\", \"item_id\"], as_index=False).min()\n",
    "    earliest_buy.rename(columns={\"time\": \"earliest_buy_time\"}, inplace=True)\n",
    "    earliest_add = data[data[\"behavior_type\"] == 3].groupby([\"user_id\", \"item_id\"], as_index=False).min()\n",
    "    earliest_add.rename(columns={\"time\": \"earliest_add_time\"}, inplace=True)\n",
    "    feature = pd.merge(earliest_buy, earliest_look, how=\"left\", on=[\"user_id\", \"item_id\"])\n",
    "    feature[\"earliest_user_item_timedelta_look_to_buy\"] = (feature[\"earliest_buy_time\"] - feature[\"earliest_look_time\"]).dt.total_seconds()/360000\n",
    "    feature = feature[feature[\"earliest_user_item_timedelta_look_to_buy\"] >= 0]\n",
    "    feature = feature[[\"user_id\", \"item_id\", \"earliest_look_time\", \"earliest_buy_time\", \"earliest_user_item_timedelta_look_to_buy\"]]\n",
    "    data = pd.merge(feature, data, how=\"left\", on=[\"user_id\", \"item_id\"])\n",
    "    data = data[(data[\"behavior_type\"] == 1)&(data[\"time\"] <= data[\"earliest_buy_time\"])]\n",
    "    data[\"item_look_counts_before_buy\"] = 1\n",
    "    item_look_counts_before_buy = data[[\"user_id\", \"item_id\", \"item_look_counts_before_buy\"]].groupby([\"user_id\", \"item_id\"], as_index=False).count()\n",
    "    feature = pd.merge(feature, item_look_counts_before_buy, how=\"left\", on=[\"user_id\", \"item_id\"])\n",
    "    return feature[[\"user_id\", \"item_id\", \"item_look_counts_before_buy\", \"earliest_user_item_timedelta_look_to_buy\"]]\n",
    "\n",
    "\n",
    "def user_category_action_total_counts(data):\n",
    "    data[\"user_category_action_total_counts\"] = 1\n",
    "    feature = data[[\"user_id\", \"item_category\", \"user_category_action_total_counts\"]].groupby([\"user_id\", \"item_category\"], as_index=False).count()\n",
    "    return feature\n",
    "\n",
    "def user_category_type_counts(data, type, name):\n",
    "    data[name] = 1\n",
    "    feature = data[data[\"behavior_type\"] == type][[\"user_id\", \"item_category\", name]].groupby([\"user_id\", \"item_category\"], as_index=False).count()\n",
    "    return feature\n",
    "# the last time\n",
    "def user_category_last_type_time(data, type, name):\n",
    "    feature = data[data[\"behavior_type\"] == type][[\"user_id\", \"item_category\", \"time\"]].groupby([\"user_id\", \"item_category\"], as_index=False).max()\n",
    "    feature.rename(columns={\"time\": name}, inplace=True)\n",
    "    return feature\n",
    "\n",
    "def user_category_look_to_buy(data):\n",
    "    buy_user_item = data[data[\"behavior_type\"] == 4][[\"user_id\", \"item_category\"]].drop_duplicates()\n",
    "\n",
    "    data = pd.merge(buy_user_item, data, how=\"left\", on=[\"user_id\", \"item_category\"])[[\"user_id\", \"item_category\", \"behavior_type\", \"time\"]]\n",
    "\n",
    "    earliest_look = data[data[\"behavior_type\"] == 1].groupby([\"user_id\", \"item_category\"], as_index=False).min()\n",
    "    earliest_look.rename(columns={\"time\": \"earliest_look_time\"}, inplace=True)\n",
    "\n",
    "    earliest_buy = data[data[\"behavior_type\"] == 4].groupby([\"user_id\", \"item_category\"], as_index=False).min()\n",
    "    earliest_buy.rename(columns={\"time\": \"earliest_buy_time\"}, inplace=True)\n",
    "\n",
    "    earliest_add = data[data[\"behavior_type\"] == 3].groupby([\"user_id\", \"item_category\"], as_index=False).min()\n",
    "    earliest_add.rename(columns={\"time\": \"earliest_add_time\"}, inplace=True)\n",
    "\n",
    "    feature = pd.merge(earliest_buy, earliest_look, how=\"left\", on=[\"user_id\", \"item_category\"])\n",
    "    feature[\"earliest_user_category_timedelta_look_to_buy\"] = (feature[\"earliest_buy_time\"] - feature[\"earliest_look_time\"]).dt.total_seconds()/3600\n",
    "    feature = feature[feature[\"earliest_user_category_timedelta_look_to_buy\"] >= 0]\n",
    "    feature = feature[[\"user_id\", \"item_category\", \"earliest_look_time\", \"earliest_buy_time\", \"earliest_user_category_timedelta_look_to_buy\"]]\n",
    "\n",
    "    data = pd.merge(feature, data, how=\"left\", on=[\"user_id\", \"item_category\"])\n",
    "    data = data[(data[\"behavior_type\"] == 1)&(data[\"time\"] <= data[\"earliest_buy_time\"])]\n",
    "    data[\"category_look_counts_before_buy\"] = 1\n",
    "    category_look_counts_before_buy = data[[\"user_id\", \"item_category\", \"category_look_counts_before_buy\"]].groupby([\"user_id\", \"item_category\"], as_index=False).count()\n",
    "    feature = pd.merge(feature, category_look_counts_before_buy, how=\"left\", on=[\"user_id\", \"item_category\"])\n",
    "\n",
    "    return feature[[\"user_id\", \"item_category\", \"category_look_counts_before_buy\", \"earliest_user_category_timedelta_look_to_buy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94fcc035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_user(data1, data2):\n",
    "    data = pd.merge(data1, data2, how=\"left\", on=\"user_id\")\n",
    "    return data\n",
    "def merge_item(data1, data2):\n",
    "    data = pd.merge(data1, data2, how=\"left\", on=\"item_id\")\n",
    "    return data\n",
    "def merge_category(data1, data2):\n",
    "    data = pd.merge(data1, data2, how=\"left\", on=\"item_category\")\n",
    "    return data\n",
    "def merge_user_item(data1, data2):\n",
    "    data = pd.merge(data1, data2, how=\"left\", on=[\"user_id\", \"item_id\"])\n",
    "    return data\n",
    "def merge_user_category(data1, data2):\n",
    "    data = pd.merge(data1, data2, how=\"left\", on=[\"user_id\", \"item_category\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13e1dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature function\n",
    "def get_feature(predict_date):\n",
    "    train_data = user_data[user_data[\"time\"] < predict_date]\n",
    "\n",
    "    ui2 = user_item_type_counts(train_data, 1, \"user_item_look_counts\")\n",
    "\n",
    "    ui5 = user_item_type_counts(train_data, 4, \"user_item_buy_counts\")\n",
    "\n",
    "    ui6 = user_item_last_type_time(train_data, 1, \"user_item_last_look_time\")\n",
    "\n",
    "    ui7 = user_item_last_type_time(train_data, 2, \"user_item_last_like_time\")\n",
    "\n",
    "    ui8 = user_item_last_type_time(train_data, 3, \"user_item_last_add_time\")\n",
    "\n",
    "    ui9 = user_item_last_type_time(train_data, 4, \"user_item_last_buy_time\")\n",
    "\n",
    "    ui10 = user_item_look_to_buy(train_data)\n",
    "\n",
    "    uc2 = user_category_type_counts(train_data, 1, \"user_category_look_counts\")\n",
    "\n",
    "    uc5 = user_category_type_counts(train_data, 4, \"user_category_buy_counts\")\n",
    "\n",
    "    uc6 = user_category_last_type_time(train_data, 1, \"user_category_last_look_time\")\n",
    "\n",
    "    uc7 = user_category_last_type_time(train_data, 2, \"user_category_last_like_time\")\n",
    "\n",
    "    uc8 = user_category_last_type_time(train_data, 3, \"user_category_last_add_time\")\n",
    "\n",
    "    uc9 = user_category_last_type_time(train_data, 4, \"user_category_last_buy_time\")\n",
    "\n",
    "    uc10 = user_category_look_to_buy(train_data)\n",
    "    \n",
    "\n",
    "    train_data = train_data[[\"user_id\", \"item_id\", \"item_category\"]].drop_duplicates()\n",
    "\n",
    "    train_data = reduce(merge_user_item, [train_data, ui2, ui5, ui6, ui7, ui8, ui9, ui10])\n",
    "    train_data = reduce(merge_user_category, [train_data, uc2, uc5, uc6, uc7, uc8, uc9, uc10])\n",
    "\n",
    "    train_data[\"user_item_last_look_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_item_last_look_time\"]).dt.total_seconds()/3600\n",
    "    train_data[\"user_item_last_like_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_item_last_like_time\"]).dt.total_seconds()/3600\n",
    "    train_data[\"user_item_last_add_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_item_last_add_time\"]).dt.total_seconds()/3600\n",
    "    train_data[\"user_item_last_buy_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_item_last_buy_time\"]).dt.total_seconds()/3600\n",
    "    train_data[\"user_category_last_look_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_category_last_look_time\"]).dt.total_seconds()/3600\n",
    "    train_data[\"user_category_last_like_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_category_last_like_time\"]).dt.total_seconds()/3600\n",
    "    train_data[\"user_category_last_add_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_category_last_add_time\"]).dt.total_seconds()/3600\n",
    "    train_data[\"user_category_last_buy_to_now\"] = (pd.to_datetime(predict_date) - train_data[\"user_category_last_buy_time\"]).dt.total_seconds()/3600\n",
    "\n",
    "    drop_columns = [\"user_item_last_look_time\", \"user_item_last_like_time\", \"user_item_last_add_time\", \"user_item_last_buy_time\"]\n",
    "    drop_columns += [\"user_category_last_look_time\", \"user_category_last_like_time\", \"user_category_last_add_time\", \"user_category_last_buy_time\"]\n",
    "    train_data = train_data.drop(drop_columns, axis=1)\n",
    "\n",
    "    fill_columns = [\"user_item_look_counts\", \"user_item_buy_counts\"]\n",
    "    fill_columns += [\"user_category_look_counts\", \"user_category_buy_counts\"]\n",
    "    train_data[fill_columns] = train_data[fill_columns].fillna(0)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "124d7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = get_feature(\"2014-12-17\")\n",
    "data_eval = get_feature(\"2014-12-18\")\n",
    "data_test = get_feature(\"2014-12-19\")\n",
    "data_train.to_csv(\"./data/data_train.csv\", index=False)\n",
    "data_eval.to_csv(\"./data/data_eval.csv\", index=False)\n",
    "data_test.to_csv(\"./data/data_test.csv\", index=False)\n",
    "user_data.to_csv(\"./data/user_data.csv\", index=False)\n",
    "item_data.to_csv(\"./data/item_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0f2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
